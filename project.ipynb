{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project \n",
    "\n",
    "FAIN Thony  \n",
    "LONCHAMBON Alexis  "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install Commands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install tqdm\n",
    "# !pip install time\n",
    "\n",
    "# %pip install --force-reinstall -v \"ipywidgets == 7.7.2\"\n",
    "# %pip install --force-reinstall -v \"jupyterlab_widgets == 1.1.1\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ALL IMPORTS FOR CODE \n",
    "\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "import json\n",
    "import math\n",
    "import requests\n",
    "import shutil\n",
    "import PIL.Image\n",
    "import webcolors\n",
    "from types import SimpleNamespace\n",
    "from PIL.ExifTags import TAGS\n",
    "from IPython.display import display\n",
    "from tqdm import tqdm\n",
    "from tqdm.notebook import tqdm_notebook\n",
    "from pandas import json_normalize\n",
    "from IPython.display import Image, HTML\n",
    "from SPARQLWrapper import SPARQLWrapper, JSON\n",
    "import matplotlib.pyplot as plot\n",
    "from sklearn.cluster import KMeans, MiniBatchKMeans"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## CLUSTERING\n",
    "\n",
    "# Numbers of color clusters for classification\n",
    "NUM_CLUSTERS = 3\n",
    "\n",
    "\n",
    "## DATA\n",
    "\n",
    "#Database names\n",
    "DB_NAME = \"db.json\"\n",
    "IMG_DB_NAME = \"db_images.json\"\n",
    "\n",
    "#Image paths\n",
    "IMG_FOLDER = \"img\"\n",
    "PLT_FOLDER = \"plt\"\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Global Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ALL GLOBAL FUNCITONS\n",
    "\n",
    "def path_to_image_html(path):\n",
    "    '''Transforms an url to an image balise for displaying'''\n",
    "    return '<img width=\"500\" src=\"'+ path + '\"/>'\n",
    "\n",
    "def format_exif(data):\n",
    "    '''Formats exifs to HTML display'''\n",
    "    out = \"\"\n",
    "    for tag, value in data.items():\n",
    "        if tag in TAGS:\n",
    "            out+=f\"{TAGS[tag]}: {value}<br>\"\n",
    "    return out\n",
    "\n",
    "def closest_color(col):\n",
    "    '''Returns the name of the closest color'''\n",
    "    min_colours = {}\n",
    "    for key, name in webcolors.CSS3_HEX_TO_NAMES.items():\n",
    "        r_c, g_c, b_c = webcolors.hex_to_rgb(key)\n",
    "        rd = (r_c - col[0]) ** 2\n",
    "        gd = (g_c - col[1]) ** 2\n",
    "        bd = (b_c - col[2]) ** 2\n",
    "        min_colours[(rd + gd + bd)] = name\n",
    "    return min_colours[min(min_colours.keys())]\n",
    "\n",
    "def format_exif_json(data):\n",
    "    '''Formats exifs to a dict for JSON parsing'''\n",
    "\n",
    "    if(not data) :\n",
    "        return None\n",
    "    \n",
    "    #This creates a object flexible enough to add attributes dynamically\n",
    "    out = SimpleNamespace()\n",
    "\n",
    "    for tag, value in data.items():\n",
    "        if tag in TAGS:\n",
    "            tagS = TAGS[tag]\n",
    "\n",
    "            #Some tags are ignored because they contain lots of useless bytes values that are a pain to format and cannot be used in our project anyway\n",
    "            if tagS in [\"MakerNote\", \"UserComment\", \"InterColorProfile\", \"ComponentsConfiguration\"]:\n",
    "                continue\n",
    "\n",
    "            #Some string values contain empty\n",
    "            if isinstance(value, str):\n",
    "                value = value.rstrip('\\x00').rstrip('\\u0000')\n",
    "\n",
    "            # add attribute to our object\n",
    "            setattr(out,tagS, value)\n",
    "    #We need to return it as a dict for JSON parsing\n",
    "    return out.__dict__\n",
    "\n",
    "def get_colors(path):\n",
    "    '''Returns a plot with the colors'''\n",
    "    if not os.path.exists(PLT_FOLDER):\n",
    "        # Create a new directory because it does not exist\n",
    "        os.makedirs(PLT_FOLDER)\n",
    "    if not os.path.exists(f\"{PLT_FOLDER}/{IMG_FOLDER}\"):\n",
    "        # Create a new directory because it does not exist\n",
    "        os.makedirs(f\"{PLT_FOLDER}/{IMG_FOLDER}\")\n",
    "\n",
    "    #Open image\n",
    "    imgfile = PIL.Image.open(path).convert('RGBA')\n",
    "\n",
    "    #We want a certain number of dominant colors\n",
    "    numClusters = NUM_CLUSTERS\n",
    "\n",
    "    try:\n",
    "        plot.clf()\n",
    "\n",
    "        # Resize to speed up image handling\n",
    "        imgfile = imgfile.resize((512,512), PIL.Image.Resampling.LANCZOS)\n",
    "\n",
    "        # Convert to 2D array\n",
    "        imgfile = np.array(imgfile)\n",
    "        w, h, d = tuple(imgfile.shape)\n",
    "        image_array = np.reshape(imgfile, (w * h, d))\n",
    "\n",
    "        # numarray = np.array(imgfile.getdata(), np.uint8)\n",
    "\n",
    "        #Clustering with MiniBatchKmeans\n",
    "        clusters = MiniBatchKMeans(n_clusters=numClusters, random_state=0, n_init=2)\n",
    "        # clusters = KMeans(n_clusters=numClusters, random_state=0, n_init=2, n_jobs=4)\n",
    "        clusters.fit(image_array)\n",
    "        npbins = np.arange(0, numClusters+1)\n",
    "        histogram = np.histogram(clusters.labels_, bins=npbins)\n",
    "        labels = np.unique(clusters.labels_)\n",
    "        barlist = plot.bar(labels, histogram[0])\n",
    "        for i in range(numClusters):\n",
    "            barlist[i].set_color(\n",
    "                \"#%02x%02x%02x\"\n",
    "                % (\n",
    "                    math.ceil(clusters.cluster_centers_[i][0]),\n",
    "                    math.ceil(clusters.cluster_centers_[i][1]),\n",
    "                    math.ceil(clusters.cluster_centers_[i][2]),\n",
    "                )\n",
    "            )\n",
    "        plot.savefig(f\"{PLT_FOLDER}/{path}\")\n",
    "        return clusters\n",
    "    except Exception as inst:\n",
    "        print(f\"RIP for {path} : {inst}\") \n",
    "        return None\n",
    "        \n",
    "            \n",
    "def download_image(url):\n",
    "    '''Downloads the image from an url to the img path'''\n",
    "\n",
    "\n",
    "    filepath = os.path.join(IMG_FOLDER, os.path.basename(url))\n",
    "\n",
    "    #creates the directory to avoid a crash (I love python...)\n",
    "    if not os.path.exists(IMG_FOLDER):\n",
    "        # Create a new directory because it does not exist\n",
    "        os.makedirs(IMG_FOLDER)\n",
    "        # print(\"The new directory is created!\")\n",
    "    headers = {\"User-Agent\": \"Mozilla/5.0\"}\n",
    "\n",
    "    #Ignore the download if the file exists\n",
    "    if os.path.isfile(filepath) :\n",
    "        return filepath\n",
    "\n",
    "    #Download code\n",
    "    request = requests.get(url, allow_redirects=True, headers=headers, stream=True)\n",
    "    if request.status_code == 200:\n",
    "        with open(filepath, \"wb\") as image:\n",
    "            request.raw.decode_content = True\n",
    "            shutil.copyfileobj(request.raw, image)\n",
    "    return filepath"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Initialisation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting the images and setting up the database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "endpoint_url = \"https://query.wikidata.org/sparql\"\n",
    "imgmax = 1000\n",
    "\n",
    "# Get cities\n",
    "query = \"\"\"SELECT DISTINCT ?planeLabel ?entry ?image {\n",
    "  ?plane wdt:P31 wd:Q15056993;\n",
    "               wdt:P729 ?entry;\n",
    "               wdt:P729 ?retirement;\n",
    "               wdt:P18 ?image.\n",
    "      \n",
    "  SERVICE wikibase:label { bd:serviceParam wikibase:language \"fr\". }\n",
    "} LIMIT 1000\"\"\"\n",
    "\n",
    "#get the results from the query from wikidata\n",
    "def get_results(endpoint_url, query):\n",
    "    user_agent = \"WDQS-example Python/%s.%s\" % (\n",
    "        sys.version_info[0],\n",
    "        sys.version_info[1],\n",
    "    )\n",
    "    sparql = SPARQLWrapper(endpoint_url, agent=user_agent)\n",
    "    sparql.setQuery(query)\n",
    "    sparql.setReturnFormat(JSON)\n",
    "    return sparql.query().convert()\n",
    "\n",
    "\n",
    "#array for dataframe\n",
    "array = []\n",
    "\n",
    "#array for JSON formatting\n",
    "db = []\n",
    "results = get_results(endpoint_url, query)\n",
    "res = results[\"results\"][\"bindings\"]\n",
    "i = 0\n",
    "\n",
    "#Parsing all results \n",
    "for result in tqdm(res):\n",
    "    i+=1\n",
    "\n",
    "    #Weird formats are ignored.\n",
    "    filename, file_extension = os.path.splitext(os.path.basename(result[\"image\"][\"value\"]))\n",
    "    if file_extension not in [\".png\", \".jpg\"] :\n",
    "        continue\n",
    "\n",
    "    #Download and get image exif data\n",
    "    path = download_image(result[\"image\"][\"value\"])\n",
    "    img = PIL.Image.open(path)\n",
    "    exif_data = img._getexif()\n",
    "    \n",
    "    #Parse data for JSON DB\n",
    "    db.append(\n",
    "        {\n",
    "            \"name\" : result[\"planeLabel\"][\"value\"],\n",
    "            \"img\" : path,\n",
    "            \"width\" : img.width,\n",
    "            \"height\" : img.height,\n",
    "            \"orientation\" : (\"Paysage\" if img.width > img.height else \"Portrait\"),\n",
    "            \"tags\" : format_exif_json(exif_data)\n",
    "        }\n",
    "    )\n",
    "\n",
    "    #Parse data for dataframe display\n",
    "    array.append(\n",
    "        (\n",
    "            result[\"planeLabel\"][\"value\"],\n",
    "            result[\"entry\"][\"value\"],\n",
    "            path,\n",
    "            img.width,\n",
    "            img.height,\n",
    "            (\"Paysage\" if img.width > img.height else \"Portrait\"),\n",
    "            exif_data\n",
    "        )\n",
    "    )\n",
    "\n",
    "\n",
    "dataframe = pd.DataFrame(array, columns=[\"planeLabel\", \"entry\", \"image\", \"width\", \"height\", \"orientation\", \"data\"])\n",
    "dataframe = dataframe.astype(\n",
    "    dtype={\"planeLabel\": \"<U200\", \"entry\" : \"<U200\", \"image\": \"<U200\", \"width\": \"int64\", \"height\": \"int64\", \"orientation\" : \"<U200\"}\n",
    ")\n",
    "# srt = dataframe.sort_values(\"data\")\n",
    "\n",
    "\n",
    "# Serializing json\n",
    "json_object = json.dumps(db, indent=4, default=lambda o: f\"{o}\")\n",
    " \n",
    "# Writing to db.json\n",
    "with open(DB_NAME, \"w\") as outfile:\n",
    "    outfile.write(json_object)\n",
    "\n",
    "#HTML Display\n",
    "pd.set_option('display.max_colwidth', 100)\n",
    "\n",
    "# HTML(srt.to_html(escape=False ,formatters=dict(image=path_to_image_html)))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filtering data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# We remove data if there are no tags\n",
    "filter1 = dataframe[\"data\"] != \"None\"\n",
    "filtered = dataframe.where(filter1).dropna()\n",
    "# HTML(filtered.to_html(escape=False ,formatters=dict(image=path_to_image_html)))\n",
    "# filtered\n",
    "mapped = filtered\n",
    "# bars = get_colors(mapped[\"image\"])\n",
    "# mapped[\"bars\"] = mapped['image'].apply(lambda x: get_colors(x))\n",
    "# mapped[\"data\"] = mapped['data'].apply(lambda x: format_exif(x))\n",
    "# mapped\n",
    "# HTML(mapped.to_html(escape=False ,formatters=dict(image=path_to_image_html)))\n",
    "mapped\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dominant Color Annotation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the parsed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Opening JSON file\n",
    "f = open(DB_NAME)\n",
    "  \n",
    "# returns JSON object as a dictionary\n",
    "data = json.load(f)\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dominant Color annotation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for entry in tqdm(data):\n",
    "    # Get image path\n",
    "    path = entry[\"img\"]\n",
    "\n",
    "    # Process image dominant colors with Kmeans\n",
    "    clusters = get_colors(path)\n",
    "\n",
    "    # If it worked and did not crash and burned, tag the image with the dominant colors\n",
    "    if clusters:\n",
    "        i = 0\n",
    "        colorlist = []\n",
    "\n",
    "        for color in clusters.cluster_centers_:\n",
    "            c = {}\n",
    "            c[\"R\"] = int(color[0])\n",
    "            c[\"G\"] = int(color[1])\n",
    "            c[\"B\"] = int(color[2])\n",
    "\n",
    "            colorlist.append(c)\n",
    "        entry[\"colors\"] = colorlist\n",
    "\n",
    "    # Serializing json\n",
    "    json2 = json.dumps(data, indent=4, default=lambda o: f\"{o}\")\n",
    "    \n",
    "    # Writing to db_images.json\n",
    "    with open(IMG_DB_NAME, \"w\") as outfile:\n",
    "        outfile.write(json2)\n",
    "\n",
    "    #HTML Display settings\n",
    "    pd.set_option('display.max_colwidth', 100)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Open Color-Tagged database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Opening JSON file\n",
    "f = open(IMG_DB_NAME)\n",
    "  \n",
    "# returns JSON object as \n",
    "# a dictionary\n",
    "data = json.load(f)\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification and Prediction"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import random\n",
    "\n",
    "results = []\n",
    "array = []\n",
    "predict = []\n",
    "favList = [\"likes\", \"yikes\"]\n",
    "outList = [\"training\", \"predict\"]\n",
    "for line in data:\n",
    "    # Extract colors\n",
    "    c1 = line[\"colors\"][0]\n",
    "    c1 =  closest_color((c1[\"R\"], c1[\"G\"], c1[\"B\"]))\n",
    "    c2 = line[\"colors\"][1]\n",
    "    c2 =  closest_color((c2[\"R\"], c2[\"G\"], c2[\"B\"]))\n",
    "    c3 = line[\"colors\"][2]\n",
    "    c3 =  closest_color((c3[\"R\"], c3[\"G\"], c3[\"B\"]))\n",
    "\n",
    "    #Extract exif\n",
    "    exif = line[\"tags\"]\n",
    "    \n",
    "    #We get rid of non-exifed data\n",
    "    if(exif):\n",
    "        Make = exif[\"Make\"] if \"Make\" in exif else None\n",
    "        ResolutionUnit = exif[\"ResolutionUnit\"] if \"ResolutionUnit\" in exif else None\n",
    "        Model = exif[\"Model\"] if \"Model\" in exif else None\n",
    "        XResolution = exif[\"XResolution\"] if \"XResolution\" in exif else None\n",
    "        YResolution = exif[\"YResolution\"] if \"YResolution\" in exif else None\n",
    "        ISOSpeedRatings = exif[\"ISOSpeedRatings\"] if \"ISOSpeedRatings\" in exif else None\n",
    "\n",
    "        # 10 out of 250 images will be used for prediction. The rest is training\n",
    "        if(random.randint(0, 250) < 100):\n",
    "            predict.append(\n",
    "                (\n",
    "                    c1,\n",
    "                    c2,\n",
    "                    c3,\n",
    "                    Make,\n",
    "                    ResolutionUnit,\n",
    "                    Model,\n",
    "                    XResolution,\n",
    "                    YResolution,\n",
    "                    ISOSpeedRatings,\n",
    "                    line[\"orientation\"]\n",
    "                )\n",
    "            )\n",
    "        else:\n",
    "            array.append(\n",
    "                (\n",
    "                    c1,\n",
    "                    c2,\n",
    "                    c3,\n",
    "                    Make,\n",
    "                    ResolutionUnit,\n",
    "                    Model,\n",
    "                    XResolution,\n",
    "                    YResolution,\n",
    "                    ISOSpeedRatings,\n",
    "                    line[\"orientation\"]\n",
    "                )\n",
    "            )\n",
    "            # We randomly like or not an image. Later, we will chose the images we like (maybe)\n",
    "            results.append(random.choices(favList, weights= [1, 10]))\n",
    "\n",
    "# Get dataframe for training and predict for prediction\n",
    "dataframe = pd.DataFrame(array, columns=[\"color1\", \"color2\", \"color3\", \"Make\", \"ResolutionUnit\", \"Model\", \"XResolution\", \"YResolution\", \"ISOSpeedRatings\", \"orientation\"])\n",
    "predict = pd.DataFrame(predict, columns=[\"color1\", \"color2\", \"color3\", \"Make\", \"ResolutionUnit\", \"Model\", \"XResolution\", \"YResolution\", \"ISOSpeedRatings\", \"orientation\"])\n",
    "\n",
    "results = pd.DataFrame(results, columns=[\"Favorite\"])\n",
    "encoded = pd.DataFrame()\n",
    "pred_en = pd.DataFrame()\n",
    "en_resu = pd.DataFrame()\n",
    "# generating numerical labels for colors\n",
    "le11 = LabelEncoder()\n",
    "encoded[\"color1_en\"] = le11.fit_transform(dataframe[\"color1\"])\n",
    "pred_en[\"color1_en\"] = le11.fit_transform(predict[\"color1\"])\n",
    "le12 = LabelEncoder()\n",
    "encoded[\"color2_en\"] = le12.fit_transform(dataframe[\"color2\"])\n",
    "pred_en[\"color2_en\"] = le11.fit_transform(predict[\"color2\"])\n",
    "le13 = LabelEncoder()\n",
    "encoded[\"color3_en\"] = le13.fit_transform(dataframe[\"color3\"])\n",
    "pred_en[\"color3_en\"] = le11.fit_transform(predict[\"color3\"])\n",
    "\n",
    "# generating numerical labels for Make\n",
    "le2 = LabelEncoder()\n",
    "encoded[\"Make_en\"] = le2.fit_transform(dataframe[\"Make\"])\n",
    "pred_en[\"Make_en\"] = le2.fit_transform(predict[\"Make\"])\n",
    "\n",
    "# Generating ResolutionUnit labels\n",
    "le_ResolutionUnit = LabelEncoder()\n",
    "encoded[\"ResolutionUnit_en\"] = le_ResolutionUnit.fit_transform(dataframe[\"ResolutionUnit\"])\n",
    "pred_en[\"ResolutionUnit_en\"] = le_ResolutionUnit.fit_transform(predict[\"ResolutionUnit\"])\n",
    "\n",
    "# Generating Model labels\n",
    "le_Model = LabelEncoder()\n",
    "encoded[\"Model_en\"] = le_Model.fit_transform(dataframe[\"Model\"])\n",
    "pred_en[\"Model_en\"] = le_Model.fit_transform(predict[\"Model\"])\n",
    "\n",
    "# Generating XResolution labels\n",
    "le_XResolution = LabelEncoder()\n",
    "encoded[\"XResolution_en\"] = le_XResolution.fit_transform(dataframe[\"XResolution\"])\n",
    "pred_en[\"XResolution_en\"] = le_XResolution.fit_transform(predict[\"XResolution\"])\n",
    "\n",
    "# Generating YResolution labels\n",
    "le_YResolution = LabelEncoder()\n",
    "encoded[\"YResolution_en\"] = le_YResolution.fit_transform(dataframe[\"YResolution\"])\n",
    "pred_en[\"YResolution_en\"] = le_YResolution.fit_transform(predict[\"YResolution\"])\n",
    "\n",
    "# Generating ISOSpeedRatings labels\n",
    "le_ISOSpeedRatings = LabelEncoder()\n",
    "encoded[\"ISOSpeedRatings_en\"] = le_ISOSpeedRatings.fit_transform(dataframe[\"ISOSpeedRatings\"])\n",
    "pred_en[\"ISOSpeedRatings_en\"] = le_ISOSpeedRatings.fit_transform(predict[\"ISOSpeedRatings\"])\n",
    "\n",
    "# generating numerical labels\n",
    "le3 = LabelEncoder()\n",
    "encoded[\"orientation_en\"] = le2.fit_transform(dataframe[\"orientation\"])\n",
    "pred_en[\"orientation_en\"] = le2.fit_transform(predict[\"orientation\"])\n",
    "\n",
    "\n",
    "# generating numerical labels\n",
    "le_res = LabelEncoder()\n",
    "en_resu[\"Favorite_en\"] = le_res.fit_transform(results[\"Favorite\"])\n",
    "\n",
    "# dataframe \n",
    "# encoded\n",
    "\n",
    "\n",
    "dataframe.join(encoded).join(results).join(en_resu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The is the data to be predicted later\n",
    "predict.join(pred_en)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classifier Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We set our classifier\n",
    "\n",
    "#Here I decided to use the RandomForestClassifier but I wanted to try the DecisionTreeClassifier too as I used it in the past. But I lacked time.\n",
    "#I also increased Max Depth to have more accurate answers given the amount of inputs. More depth would have overfitted.\n",
    "rfc = RandomForestClassifier(\n",
    "    n_estimators=10,\n",
    "    max_depth=5,\n",
    "    random_state=0,\n",
    ")\n",
    "\n",
    "#And now get fit, get swole\n",
    "rfc = rfc.fit(encoded.values, en_resu.values.ravel())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classifier Visualisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import graphviz\n",
    "import pydotplus\n",
    "from sklearn import tree\n",
    "\n",
    "# We now display all our decision trees\n",
    "for i in range(10):\n",
    "    dot_data = tree.export_graphviz(\n",
    "        rfc.estimators_[i],\n",
    "        out_file=None,\n",
    "        feature_names=dataframe.columns,\n",
    "        filled=True,\n",
    "        rounded=True,\n",
    "        class_names=le_res.inverse_transform(en_resu.Favorite_en.unique()),\n",
    "    )\n",
    "    graph = graphviz.Source(dot_data)\n",
    "    pydot_graph = pydotplus.graph_from_dot_data(dot_data)\n",
    "    img = Image(pydot_graph.create_png())\n",
    "    display(img)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prediction part : we predict all our dataframe built previously\n",
    "prediction = rfc.predict(pred_en)\n",
    "\n",
    "# We then reverse transform the prediction tag for a readable output\n",
    "prediction2 = le_res.inverse_transform(prediction)\n",
    "prediction = pd.DataFrame(prediction, columns=[\"Fav\"])\n",
    "\n",
    "# Display output to jupyter\n",
    "prediction2 = pd.DataFrame(prediction2)\n",
    "predict.join(prediction).join(prediction2).sort_values(\"Fav\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "8b7946ee73675b5668978e3bbcf853598e979dfcc8d2f615788a35ee60005c41"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
